\documentclass[11pt]{article}
\usepackage{color, array, graphics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage[utf8]{inputenc}

\begin{document}
\lstset{stringstyle=\ttfamily,
	showstringspaces=false,
	basicstyle=\small}

\begin{center} Alexander Garcia \hfill June 12, 2017 \\ Assignment-3 \end{center}

\medskip

\begin{enumerate}

	\item
	\begin{enumerate}[(a)]

		\item \textbf{False}.

		Because $\mathbf{A}$ is nonsingular, there are a number of properties that are associated with it. One of these is the fact that $\mathbf{A}$
		has a unique solution for all $\mathbf{b}$. Likewise if $\mathbf{b = 0}$, then $\mathbf{x}$ is necessarily the 0
		vector.

		This question was done assuming that $\mathbf{A}=n\times n \rightarrow \mathbf{b}=1\times n$. If $\mathbf{b}$ is of size $m\neq n$, then $\mathbf{Ax=b}$ has
		either zero or infinitely many solutions. However, this does not depend on the contents of $\mathbf{b}$, but rather the initial parameters of the problem. \\

		\item \textbf{False}.

		This can be proven false simply by taking the determinant of a $3\times 3$ matrix.
		$$det(\mathbf{A}) = a_{11}(a_{22}a_{33} + a_{23}a_{32}) - a_{12}(a_{21}a_{33} + a_{23}a_{31}) + a_{13}(a_{21}a_{32} + a_{22}a_{31})$$

		As there are terms in this equation that are not dependant on $a_{ii}$, a matrix with a zero on the principle diagonal is not necessarily singular.\\

		\item \textbf{False}.

		The conditioning of a matrix $\mathbf{A}$ is related to the largest sum of a single column (the 1-norm). The partial pivoting algorithm scans the rest of the
		column below the current pivot point $(a_{kk})$ for the largest value.

		A well-conditioned matrix would simply have small column sums, which does not necessarily affect the location of the largest values in each row, which in turn
		does not affect the need for partial pivoting. \\

		\item \textbf{True}.

		By definition, the condition number of a matrix $\mathbf{A = ||A||\ ||A^{-1}||}$. Therefore, the condition number of
		$\mathbf{A^{-1} = ||A^{-1}||\ ||(A^{-1})^{-1}|| = ||A^{-1}||\ ||A||}$. As $||A||,\ ||A^{-1}||$ are scalar values, the two products are the same. \\

		\item \textbf{False}.

		If all lines are parallel, there are zero intersections, and therefore zero solutions. If all lines are the same, there are
		infinitely many intersections, and therefore infinitely many solutions. If all lines intersect at exactly one point, there is
		exactly one solution. There are no other possibilities, as two linear equations cannot intersect at 2 points unless they
		intersect at infinitely many points.\\

		\item One of the reasons pivoting is essential in the G.E. method of solving matrices is to avoid potential zeroes on the principle
		diagonal. When the G.E. is being done, each element on the principle diagonal determines the multiplier for the rows below, $m_{ij}
		= \frac{a_{ij}}{a_{jj}}$. Since the diagonal element is in the denominator, it must be nonzero. This is a mathematical reason for
		pivoting.

		The other reason is a numerical one. If the current pivot point is $a_{jj}$, and this value is very small $(10^{-16})$, then the
		multiplier that results will be extremely large. If the values in column $j$ are then relatively small to $a_{jj}$, the subtraction
		will result in a loss of significance, due to not being able to express numbers such as $10^{16}-1$ accurately. Partial pivoting moves
		the row with the largest $j^{th}$ element to the current row, then computes the multiplier. This avoids the large multiplier result,
		and will aid in maintaining significance. \\

		\item The condition number is a nontrivial computation due to the computation of the norms that make up the number. In order
		compute the 1-norm, which is common, the algorithm must sum every column, the largest of which becomes the condition number.
		This alone requires $O(n^2)$ operations. The algorithm must then compute the inverse of $\mathbf{A}$, which is an even more
		expensive operation. More curcially, $\mathbf{A^{-1}}$ must be completely recomputed if $\mathbf{A}$ is even slightly
		modified. \\

		\item On its own, the relative residual does not carry much weight. A small residual would, at first glance, seem to imply low
		error, and therefore higher accuracy. However, this is not the case, and it turns out that the condition number of a matrix is
		directly involved in the accuracy of the solution. In order for a solution to be accurate, the solution should not only have
		a small relative residual, but should also be moderately well conditioned. \\

		\item
			\begin{enumerate}[i.]

				\item $det(\mathbf{cA}) = c^n*det(\mathbf{A})$

				In this case, $\mathbf{A}$ would be an $n\times n$ diagonal matrix, with every diagonal entry = 1, and $c =
				\frac{1}{2}$. The determinant of a diagonal matrix with every diagonal value = 1 is simply the identity matrix
				, and $det(\mathbf{I})$ is 1. Therefore, $det(\frac{1}{2}\mathbf{I}) =
				(\frac{1}{2})^n$. \\

				\item Assume the use of the 1-norm

				$||\mathbf{A}||_1 = \frac{1}{2}$

				Because the inverse of a diagonal matrix is found by simply inverting all diagonal elements, $||\mathbf{A}^{-1}
				||_1 = 2$

				$ K(\mathbf{A}) = ||\mathbf{A}||_1 * ||\mathbf{A^{-1}}||_1 = 1$ \\

				\item Any scalar multiple of the identity matrix will not amplify input errors. This makes intuitive sense, as
				the condition number of a matrix sets an upper bound on the maximum distortion of the result due to input
				errors. Because each element in the system $(x_1, x_2, \dots, x_n)$ only appears once in $\mathbf{A}$,
				each entry simply takes the corresponding $\mathbf{b}$ value $(a_1 = b_1, a_2 = b_2, \dots, a_n = b_n)$.
				Any error that is present in the input will be directly translated to the output. \\

			\end{enumerate}

	\end{enumerate}

	\newpage

	\item
		\begin{enumerate}[(a)]

			\item
			\[
				\begin{bmatrix}
					1 & 2 \\
					2 & 4.01
				\end{bmatrix}
				\begin{bmatrix}
					x_1 \\
					x_2
				\end{bmatrix}
				=
				\begin{bmatrix}
					3 \\
					6.01
				\end{bmatrix}
			\]

			$x_1 = 3-2x_2$

			$2(3-2x_2) + 4.01x_2 = 6.01$

			$6 + 0.01x_2 = 6.01$

			$0.01x_2 = 0.01$

			$x_2 = 1$

			$x_1 = 1$ \\

			\item

			\[
				\begin{bmatrix}
					1 & 2 \\
					2 & 4.01
				\end{bmatrix}
				\begin{bmatrix}
					-600 \\
					301
				\end{bmatrix}
				=
				\begin{bmatrix}
					2 \\
					7.1
				\end{bmatrix}
			\]

			Here,

			\[
				x^* =
				\begin{bmatrix}
					-600 \\
					301
				\end{bmatrix}
			\]
			and
			\[
				b^* =
				\begin{bmatrix}
					2 \\
					7.1
				\end{bmatrix}
			\]

			The relative forward error is defined by $\frac{||\mathbf{x^*}||_{\infty}}{||\mathbf{x}||_{\infty}}$, while the
			relative backward error
			is $\frac{||\mathbf{b^*}||_{\infty}}{||\mathbf{b}||_{\infty}}$. \\

			\begin{tabular}{ll}
				Relative Forward Error & Relative Backward Error \\
				\hline
				$||\mathbf{x^*}|| = 600$ & $||\mathbf{b^*}|| = 7.1$ \\

				$||\mathbf{x}|| = 1$ & $||\mathbf{b}|| = 6.01$ \\

				$\frac{600}{1} = 600$ & $\frac{7.1}{6.01} \approx 1.18\dots$ \\

			\end{tabular} \\

			The error magnification factor is (relative forwards error $\div$ relative backwards error).

			$E_m = 600 \div \frac{7.1}{6.01} \approx 507.89\dots$ \\

			\item $K(\mathbf{A}) = ||\mathbf{A}||\ ||\mathbf{A^{-1}}||$

			$||\mathbf{A}|| = 6.01$

			\[
				\mathbf{A^{-1}} =
				\frac{1}{0.01}
				\begin{bmatrix}
					4.01 & -2 \\
					-2 & 1
				\end{bmatrix}
			\]

			$||\mathbf{A^{-1}}|| = 201$

			$$K(\mathbf{A}) = 201*6.01 = 1208.01$$

			The poor conditioning of the matrix is a clear indication that a small change to the data will cause a large change in
			the result. In this case, the altered data that gave rise to the vector $\mathbf{x^*}$ was $\mathbf{b^*}$. The large
			condition number sets the upper bound on the relative output error, meaning that $\mathbf{x^*}$ can grow extremely
			large, without producing extremely different results in $\mathbf{b^*}$.

	\end{enumerate}

	\item

		\begin{enumerate}[(a)]

			\item Begin with the augementd matrix $\mathbf{A|b}$

				\[
					\mathbf{A|b} =
					\begin{bmatrix}
						1 & 2 & 3 & 4 & -6 \\
						2 & 3 & 7 & 9 & -17 \\
						-2& -6 & -2 & -4 & -4 \\
						1 & 0 & 11 & 15 & -34
					\end{bmatrix}
				\]

				\textbf{Step 1:} Pivot point $a_{11}$

				First, we must employ partial pivoting to ensure there are no multipliers $>$ 1. We see that row 2 has the
				largest value in column 1, so we use this as the first fow instead. The permutation matrix associated with
				swapping rows $R1$ and $R2$ is
				\[
					\mathbf{P} =
					\begin{bmatrix}
						0 & 1 & 0 & 0 \\
						1 & 0 & 0 & 0 \\
						0 & 0 & 1 & 0 \\
						0 & 0 & 0 & 1
					\end{bmatrix}
				\]

				and the matrix we are now operating on is
				\[
					\mathbf{A}
					\begin{bmatrix}
						2 & 3 & 7 & 9 & -17 \\
						1 & 2 & 3 & 4 & -6 \\
						-2& -6 & -2 & -4 & -4 \\
						1 & 0 & 11 & 15 & -34
					\end{bmatrix}
				\]

				We now calculate the multipliers needed to zero out the first column.

				\begin{center}
				\begin{tabular}{c||c||c}

					$m_{21} = \frac{a_{21}}{a_{11}} = 0.5$ &
					$m_{31} = \frac{a_{31}}{a_{11}} = -1$ &
					$m_{41} = \frac{a_{41}}{a_{11}} = 0.5$ \\
				\end{tabular} \\
				\end{center}

				Then, carry out the row operations on $R2, R3, R4$.

				\begin{center}
				\begin{tabular}{c||c||c}

					$R2 \to R2 - m_{21}R1$ &
					$R3 \to R3 - m_{31}R1$ &
					$R4 \to R4 - m_{41}R1$ \\
				\end{tabular} \\
				\end{center}

				The new matrix generatred after these operations is

				\[
					\mathbf{A}=
					\begin{bmatrix}
						2 & 3 & 7 & 9 & -17 \\
						(0.5) & 0.5 & -0.5 & -0.5 & -2.5 \\
						(-1) & -3 & -5 & 5 & -21 \\
						(0.5) & -1.5 & 7.5 & 10.5 & -25.5
					\end{bmatrix}
				\]

				where the normal zeros are replaced by the multipliers. \\

				\textbf{Step 2:} Pivot point $a_{22}$

				Again, start by swapping rows according to partial pivoting. This includes changing the positions of the
				multipliers, although their values will not be affected by the row operations that follow. It also includes
				swapping the corresonding rows in the permutation matrix. In this case, we swap rows $R2$ and $R3$.

				\[
					\mathbf{A} =
					\begin{bmatrix}
						2 & 3 & 7 & 9 & -17 \\
						(-1) & -3 & -5 & 5 & -21 \\
						(0.5) & 0.5 & -0.5 & -0.5 & -2.5 \\
						(0.5) & -1.5 & 7.5 & 10.5 & -25.5
					\end{bmatrix}
				\]
				and
				\[
					\mathbf{P} =
					\begin{bmatrix}
						0 & 1 & 0 & 0 \\
						0 & 0 & 1 & 0 \\
						1 & 0 & 0 & 0 \\
						0 & 0 & 0 & 1
					\end{bmatrix}
				\]

				Next, calculate the new multipliers. In this step there are only two, as the only rows that have items to be
				zeroed out are $R3$ and $R4$.

				\begin{center}
				\begin{tabular}{c||c}

					$m_{32} = \frac{a_{32}}{a_{22}} = -\frac{1}{6}$ &
					$m_{42} = \frac{a_{42}}{a_{22}} = 0.5$ \\
				\end{tabular} \\
				\end{center}

				Next, perform the row operations on $R3, R4$.

				\begin{center}
				\begin{tabular}{c||c}

					$R3 \to R3 - m_{32}R2$ &
					$R4 \to R4 - m_{42}R2$ \\
				\end{tabular} \\
				\end{center}

				The new matrix is

				\[
					\mathbf{A}=
					\begin{bmatrix}
						2 & 3 & 7 & 9 & -17 \\
						(-1) & -3 & -5 & 5 & -21 \\
						(0.5) & (-\frac{1}{6}) & \frac{1}{3} & \frac{1}{3} & -6 \\
						(0.5) & (0.5) & 5 & 8 & 36
					\end{bmatrix}
				\]

				again, with the normal zeros replaced by the multipliers. \\

				\textbf{Step 3:} Pivot point $a_{33}$

				In this case, we have to switch $R3$ and $R4$ in $\mathbf{A}$ and $\mathbf{P}$ according to partial pivoting.

				\[
					\mathbf{A}=
					\begin{bmatrix}
						2 & 3 & 7 & 9 & -17 \\
						(-1) & -3 & -5 & 5 & -21 \\
						(0.5) & (0.5) & 5 & 8 & 36 \\
						(0.5) & (-\frac{1}{6}) & \frac{1}{3} & \frac{1}{3} & -6 \\
					\end{bmatrix}
				\]

				and

				\[
					\mathbf{P} =
					\begin{bmatrix}
						0 & 1 & 0 & 0 \\
						0 & 0 & 1 & 0 \\
						0 & 0 & 0 & 1 \\
						1 & 0 & 0 & 0 \\
					\end{bmatrix}
				\]

				The only multiplier to be calculated is $m_{43}$
				\begin{center}
					$m_{43} = \frac{a_{43}}{a_{33}} = \frac{1}{15}$
				\end{center}

				and the only row operation to be performed is
				\begin{center}
					$R4 \to R4 - m_{43}R3$
				\end{center}

				The final matrix is

				\[
					\mathbf{A}=
					\begin{bmatrix}
						2 & 3 & 7 & 9 & -17 \\
						(-1) & -3 & -5 & 5 & -21 \\
						(0.5) & (0.5) & 5 & 8 & 36 \\
						(0.5) & (-\frac{1}{6}) & (\frac{1}{15}) & -0.2 & -8.4 \\
					\end{bmatrix}
				\] \\

				From here, we can extract both the lower and upper triangular matrices $\mathbf{L, U}$.

				\[
					\mathbf{L}=
					\begin{bmatrix}
						1 & 0 & 0 & 0 \\
						-1 & 1 & 0 & 0 \\
						0.5 & 0.5 & 1 & 0 \\
						0.5 & -\frac{1}{6} & \frac{1}{15} & 1 \\
					\end{bmatrix}
					\mathbf{U}=
					\begin{bmatrix}
						2 & 3 & 7 & 9 \\
						0 & -3 & 5 & 5 \\
						0 & 0 & 5 & 8 \\
						0 & 0 & 0 & -0.2 \\
					\end{bmatrix}
					\mathbf{P}=
					\begin{bmatrix}
						0 & 1 & 0 & 0 \\
						0 & 0 & 1 & 0 \\
						0 & 0 & 0 & 1 \\
						1 & 0 & 0 & 0 \\
					\end{bmatrix}
				\] \\

			\item When checked in MATLAB, the results of the hand calculation agree with the computed result.

				\begin{center}
					\texttt{lup\_check.m} script used to generate matrices.
				\end{center}
				\lstinputlisting{lup_check.m}

				\begin{center}
					Output of \texttt{lup\_check.m}
				\end{center}
				\lstinputlisting{lup_check.txt} \

			\item If the decomposition is correct, $\mathbf{LU == AP}$

				\[
					\mathbf{LU}=
					\begin{bmatrix}
						L_{11}U_{11} & L_{11}U_{12} & L_{11}U_{13} & L_{11}U_{14} \\
						L_{21}U_{11} & L_{21}U_{12} + L_{22}U_{22} & L_{21}U_{13} + L_{22}U_{23} &
						L_{21}U_{14} + L_{22}U_{24} \\
						L_{31}U_{11} & L_{31}U_{12} + L_{32}U_{22} & L_{31}U_{13} + L_{32}U_{23} + L_{33}U_{33} &
						L_{31}U_{14} + L_{32}U_{24} + L_{33}U_{34} \\
						L_{41}U_{11} & L_{41}U_{12} + L_{42}U_{22} & L_{41}U_{13} + L_{42}U_{23} + L_{43}U_{33} &
						L_{41}U_{14} + L_{42}U_{24} + L_{43}U_{34} + L_{44}U_{44} \\
					\end{bmatrix}
				\]

				\[
					=
					\begin{bmatrix}
						2 & 3 & 7 & 9 \\
						-2 & (-3-3) & (-7+5) & (-9+5) \\
						1 & (1.5-1.5) & (3.5 + 2.5 + 5) & (4.5 + 2.5 + 8) \\
						1 & (1.5 + 0.5) & (3.5 - \frac{5}{6} + \frac{1}{3}) & (4.5 - \frac{5}{6} + \frac{8}{15} -0.2)\\
					\end{bmatrix}
				\]

				\[
					=
					\begin{bmatrix}
						2 & 3 & 7 & 9 \\
						-2 & -6 & -2 & -4 \\
						1 & 0 & 11 & 15 \\
						1 & 2 & 3 & 4 \\
					\end{bmatrix}
				\] \\

				The product $\mathbf{AP}$ can be found analytically by taking the row operations needed to convert the identity
				matrix $\mathbf{I}$ into $\mathbf{P}$, and applying them to $\mathbf{A}$.

				$\mathbf{P} = (R1_I \leftrightarrow R2_I) \to (R2_I \leftrightarrow R3_I) \to (R3_I \leftrightarrow R4_I)$

				Performing the same operations on $\mathbf{A}$ yields

				\[
					\mathbf{AP}=
					\begin{bmatrix}
						2 & 3 & 7 & 9 \\
						-2 & -6 & -2 & -4 \\
						1 & 0 & 11 & 15 \\
						1 & 2 & 3 & 4 \\
					\end{bmatrix}
				\]

				The two matrices are equal, and there is no residual. \\

		\end{enumerate}

	\item The matrix $\mathbf{A}$ is symmetric positive definite if the determinants of all leading principle submatrices are positive.
		\[
			\mathbf{A}=
			\begin{bmatrix}
				4 & -2 & 0 \\
				-2 & 2 & -3 \\
				0 & -3 & 10 \\
			\end{bmatrix}
		\]

		Here, the determinant of the first principle submatrix is simply $4$, the second is $((4*2)-(-2*-2)) = 4$, and the third
		is $4((2*10)-(-3*-3)) + 2((-2 * 10)-0) + 0 = 4$. \\

		The Cholesky decomposition says that $\mathbf{LL^T = A}$, meaning

		\[
			\begin{bmatrix}
				l_{11} & 0 & 0 \\
				l_{21} & l_{22} & 0 \\
				l_{31} & l_{32} & l_{33} \\
			\end{bmatrix}
			\begin{bmatrix}
				l_{11} & l_{21} & l_{31} \\
				0 & l_{22} & l_{32} \\
				0 & 0 & l_{33} \\
			\end{bmatrix}
			=
			\begin{bmatrix}
				a_{11} & a_{12} & a_{13} \\
				a_{21} & a_{22} & a_{23} \\
				a_{31} & a_{32} & a_{33} \\
			\end{bmatrix}
		\]

		\textbf{Step 1:} Multiply $R1$ of $\mathbf{L}$ with columns of $\mathbf{L^T}$. This result is the first row of $\mathbf{A}$.

		\begin{tabular}{ll}
			$(l_{11})^2 = 4$ & $l_{11} = 2$ \\

			$l_{11}l_{21} = -2$ & $l_{21} = -1$ \\

			$l_{11}l_{31} = 0$ & $l_{31} = 0$ \\
		\end{tabular} \\

		\textbf{Step 2:} Multiply $R2$ of $\mathbf{L}$ with columns of $\mathbf{L^T}$. This result is the second row of $\mathbf{A}$.
		Since the first equation $(l_{21}l_{11})$ does not have any unknowns, we can skip this calculation.

		\begin{tabular}{ll}
			$(l_{21})^2 + (l_{22})^2 = 2$ & $l_{22} = 1$ \\

			$l_{21}l_{31} + l_{22}l_{32} = -3$ & $l_{32} = -3$ \\
		\end{tabular} \\

		\textbf{Step 3:} Multiply $R3$ of $\mathbf{L}$ with columns of $\mathbf{L^T}$. This result is the second row of $\mathbf{A}$.
		Since The first equation $(l_{32}l_{11})$, and the second equation $(l_{31}l_{21} + l_{32}l_{22})$ do not have any unkowns, we
		can skip them, and are left with a single calculation for $l_{33}$.

		\begin{tabular}{ll}
			$(l_{31})^2+(l_{32})^2+(l_{33})^2 = 10$ & $l_{33} = 1$ \\
		\end{tabular}

		\[
			\mathbf{L} =
			\begin{bmatrix}
				2 & 0 & 0 \\
				-1 & 1 & 0 \\
				0 & -3 & 1 \\
			\end{bmatrix}
		\]

		Just to make sure that this is a valid Cholesky decomposition,

		\[
			\begin{bmatrix}
				2 & 0 & 0 \\
				-1 & 1 & 0 \\
				0 & -3 & 1 \\
			\end{bmatrix}
			\begin{bmatrix}
				2 & -1 & 0 \\
				0 & 1 & -3 \\
				0 & 0 & 1 \\
			\end{bmatrix}
			=
			\begin{bmatrix}
				(2*2) & (2*-1) & 0 \\
				(2*-1) & (-1*-1) + (1*1) & (-3*1) \\
				0 & (-3 * 1) & (-3*-3) + (1*1) \\
			\end{bmatrix}
		\]

		\[
			=\begin{bmatrix}
				4 & -2 & 0 \\
				-2 & 2 & -3 \\
				0 & -3 & 10 \\
			\end{bmatrix}
		\] \\

	\item
		\begin{enumerate}[(a)]

		\item See below

			\begin{center}
				\texttt{myA.m} function
			\end{center}
			\lstinputlisting{myA.m} \

		\item The results of \texttt{myA.m} for size $n =5:20$ are generated by a second script, \texttt{runmyA.m}.

			To summarize,
			the condition number of $\mathbf{A}\ (\kappa(\mathbf{A}))$ grows increasingly large with $n$. This causes the relative
			error to increase as well, causing the significance of the result to deteriorate, until there is no significance left
			at $n=17$. \\

			Note: The \texttt{fprintf} command that displays the results is too long to be displayed on a single line. The ending
			variables that are used are \texttt{nE, kA, rr, scaledR}, which are the norm of the error, the condition number of
			$\mathbf{A}$, the relative residual, and the upper bound on the error, respectively.

			\begin{center}
				\texttt{runmyA.m}
			\end{center}
			\lstinputlisting{runmyA.m} \

			\begin{center}
				Output of \texttt{runmyA.m} script
			\end{center}
			\lstinputlisting{myA.txt} \

	\end{enumerate}

	\item

		\begin{enumerate}[(a)]
			\item By definition, $\mathbf{AA^{-1}} = I$.

				If $\mathbf{A}$ is an $n\times n$ matrix, then so are $\mathbf{A^{-1}}$ and $\mathbf{I}$. When the multiplication
				$\mathbf{AA^{-1}}$ is carried out, there are exactly $n$ terms that result in a $1$, and they lie on the principle
				diagonal.

				The rest of the terms are necessarily zero. The expressions that produce the diagonal ones are
				$a_{11}a^{-1}_{11} + a_{12}a^{-1}_{21} + \dots + a_{1n}a^{-1}_{n1}$ \\
				$a_{21}a^{-1}_{12} + a_{22}a^{-1}_{22} + \dots  + a_{2n}a^{-1}_{n2}$ \\
				$\vdots$ \\
				$a_{n1}a^{-1}_{1n} + \dots + a_{nn}a^{-1}_{nn}$ \\

				When taking the product of $\mathbf{A}$ and any of the columns of $\mathbf{A^{-1}}$, the result is
				\[
					\begin{bmatrix}
						a_{11}a^{-1}_{1i} + a_{12}a^{-1}_{2i} + \dots + a_{1n}a^{-1}_{ni} \\
						a_{21}a^{-1}_{1i} + a_{22}a^{-1}_{2i} + \dots + a_{2n}a^{-1}_{ni} \\
						\vdots \\
						a_{n1}a^{-1}_{1i} + a_{n2}a^{-1}_{2i} + \dots + a_{nn}a^{-1}_{ni} \\
					\end{bmatrix}
				\]

				where $i=1:n$.

				We know that the only sum that produces a 1 is of the form $a_{i1}a^{-1}_{1i} + a_{i2}a^{-1}{2i} + \dots a_{in}a^{
				-1}{ni}$.
				Clearly, there is only one of these terms int he product shown above, and the rest of the terms are necessarily zero.
				Therefore, $\mathbf{Au_i = v_i}$. \\

			\item
				The calculations for $\mathbf{A^{-1}}$ were carried out in matlab by the script \texttt{itinverse.m}. This
				script generates a random $5\times 5$ matrix, and finds the column vector $\mathbf{u_i}$ such that
				$\mathbf{A^{-1}u_i = I_i}$, where $\mathbf{I_i}$ is the $i^{th}$ column vector of the identity matrix.

				\begin{center}
					\texttt{itinverse.m} script
				\end{center}
				\lstinputlisting{itinverse.m} \

			\item The calculated values of $\mathbf{A^{-1}, I}$ are displayed below.

				\begin{center}
					Output of \texttt{itinverse.m}
				\end{center}
				\lstinputlisting{itinverse.txt} \

		\end{enumerate}

	\item The solutions to the Lorenz equations are calculated by two MATLAB files. The first, \texttt{lorenz.m}, defines the process
		for computing the next iterate using the given functions and their partial derivatives. The second, \texttt{runlorenz.m},
		contains a loop that supplies \texttt{lorenz.m} values according to Newton's method. When two successive iterates are close
		enough togher, or if the system evaluated at the current result is close enough to zero, the script stops and displays
		a solution to the system. Below are \texttt{lorenz.m}, \texttt{runlorenz.m}, and the output for the three solutions. The
		particular solution reached depends on the inital guesses provided by the user.

		\begin{center}
			\texttt{lorenz.m} function
		\end{center}
		\lstinputlisting{lorenz.m} \

		\begin{center}
			\texttt{runlorenz.m} script
		\end{center}
		\lstinputlisting{runlorenz.m} \

		\begin{center}
			Solution set 1 produced by \texttt{runlorenz.m}
		\end{center}
		\lstinputlisting{lorenz.txt} \

		\begin{center}
			Solution set 2 produced by \texttt{runlorenz.m}
		\end{center}
		\lstinputlisting{lorenz2.txt} \

		\begin{center}
			Solution set 3 produced by \texttt{runlorenz.m}
		\end{center}
		\lstinputlisting{lorenz3.txt} \

\end{enumerate}

\end{document}
